[
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Connor! I’m from Carlsbad, CA and am a data scientist.\nI started my journey into data science from the world of television news production after I had the realization that audience viewing habits impact TV coverage in a huge way. Ever since I started working on the research and data science side of the industry (rather than production), I have developed a greater appreciation for and fascination in the media landscape as a whole.\nMy largest projects to date have involved forecasting the average audience of a cable network’s programming - weeks to years ahead of time - all to be able to help decide whether special events are worth broadcasting, to sell advertisements ahead of time, and to help set the network’s overall budget. All of these facets of forecasting involve handling anomalous viewership, accounting for upcoming elections and varying political climates, monitoring the integrity of 3rd party data, and many other factors.\nWhen not performing data dives, you can often find me at the beach, playing and coaching water polo, or even reenacting the American Civil War.\nIf you would like to learn more about me, or what I do, please check out some of the links on the left - and don’t hesitate to reach out!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Connor Hanan",
    "section": "",
    "text": "I am Connor Hanan, a data scientist and television researcher.\nPassionate about the growing intersection of data analytics and television, I am interested in applying data-driven visualizations and models to explore the future of news media."
  },
  {
    "objectID": "portfolio/index.html",
    "href": "portfolio/index.html",
    "title": "Portfolio",
    "section": "",
    "text": "A side project, this was created out of my interest in tracking the daily cable news Nielsen ratings. This script was scheduled to run as a specific time each day, and would scrape the information off of a site, process the data into nice tables, and automatically email me the results. Web scraping was initially quite complex, though after this project using the rvest package is now a much more familiar process.\nClick here to check it out!"
  },
  {
    "objectID": "portfolio/index.html#data-mining-sem-project",
    "href": "portfolio/index.html#data-mining-sem-project",
    "title": "Portfolio",
    "section": "Data Mining Sem Project",
    "text": "Data Mining Sem Project\nThis project was twofold in aim: determine which insurance indicator has the largest effect on the policy charges, as well as finding which model best reflects our data. In the analysis, models of numerous variations were used, including: KNN, SVM, Decision Trees, Neural Nets, eXtreme Gradient Boosting, and Naive Bayes. Overall, the project was a deep dive into all things machine learning, from data cleaning and preparation to executing the models and checking their accuracy.\nClick here to check it out!"
  },
  {
    "objectID": "portfolio/index.html#data-viz-sem-project",
    "href": "portfolio/index.html#data-viz-sem-project",
    "title": "Portfolio",
    "section": "Data Viz Sem Project",
    "text": "Data Viz Sem Project\nAn early foray into the world of R and the Tidyverse, this project explores the competitive differences between Formula 1 teams who build thier own engines versus those who buy them from manufacturers. Though it is fairly basic analysis in nature, this project offered lots of practice in manipulating and plotting data using the tidyverse.\nClick here to check it out!"
  },
  {
    "objectID": "portfolio/index.html#event-cvg-tern-plot",
    "href": "portfolio/index.html#event-cvg-tern-plot",
    "title": "Portfolio",
    "section": "Event Cvg Tern Plot",
    "text": "Event Cvg Tern Plot\nCreated as a foray into NLP, this project utilizes the GDELT closed captioning data of cable news channels to analyze usage patterns across networks. This iteration of the project was created to analyze coverage of the Georgia Senate runoff elections in December of 2022; however, it is built in such a way that it can be used for any event - all you need is a date and a keyword to filter around.\nFor further information about this project, and to see the results, please find it in the blog post I wrote.\nClick here to check it out!"
  },
  {
    "objectID": "portfolio/index.html#data-analysis-sem-project",
    "href": "portfolio/index.html#data-analysis-sem-project",
    "title": "Portfolio",
    "section": "Data Analysis Sem Project",
    "text": "Data Analysis Sem Project\nCreated as a final project for class, this analysis dives into the electric vehicle rebates offered by New York State. Specifically, it uses the apriori algorithm to conduct association rules mining on the information to determine which vehicles were the most popular for consumers to claim the maximum rebate with. Python’s pandas, sklearn, and mlxtend libraries featured quite prominently throughout this analysis.\nClick here to check it out!"
  },
  {
    "objectID": "resume/index.html",
    "href": "resume/index.html",
    "title": "Resume",
    "section": "",
    "text": "Download current resume"
  }
]